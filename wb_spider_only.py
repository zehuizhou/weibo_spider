import re
import sys
import requests
from lxml import html
import datetime
import string
from constants import change_proxy, save_to_csv, web_header, web_url

etree = html.etree

total_page = 1


class WbSpider:
    def __init__(self, keyword, start_time, end_time, page):
        self.keyword = keyword
        self.start_time = start_time
        self.end_time = end_time
        self.page = page

    def get_web_data(self, retry_count):
        # 发起请求
        param = {
            'scope': 'ori',  # 是否原创
            'q': self.keyword,
            # 'typeall': '1',  # 全部（和scope二选一）
            'suball': '1',
            'timescope': 'custom:' + self.start_time + ':' + self.end_time,
            'Refer': 'g',
            'page': self.page
        }
        if retry_count < 0:
            sys.exit()
        try:
            with open('pro.txt', 'r') as f:
                proxy = eval(f.read())
            res_web = requests.get(url=web_url, headers=web_header, params=param, proxies=proxy, timeout=6).content.decode()
            root_web = etree.HTML(res_web)
            div_list = root_web.xpath("//div[@class='card-wrap']/div[@class='card']")
            assert div_list
        except Exception as e:
            print(e)
            change_proxy(3)
            return self.get_web_data(retry_count - 1)

        # 提取数据
        if self.page == 1:
            global total_page
            total_page = len(root_web.xpath("//ul[@class='s-scroll']/li"))
            if total_page == 0:
                total_page = 1
            print(f'总页数:{total_page}')

        div_list = root_web.xpath("//div[@class='card-wrap']/div[@class='card']")
        web_data_list = []
        for div in div_list:
            # 判断是否有“展开全文”
            is_full = div.xpath(".//div[@class='content']/p[@node-type='feed_list_content_full']")

            web_item = {}
            user_url = 'https:' + div.xpath(".//div[@class='info']/div[2]/a/@href")[0]

            web_item['微博id'] = '`' + str(div.xpath("./../@mid")[0])
            web_item['用户名'] = div.xpath(".//div[@class='info']/div[2]/a/text()")[0]

            web_item['个人主页链接'] = user_url
            web_item['用户id'] = re.findall('.*weibo.com/(.*)refer_flag', user_url)[0].replace('?', '')

            content = div.xpath("string(.//div[@class='content']/p[@node-type='feed_list_content_full'])") if is_full \
                else div.xpath("string(.//div[@class='content']/p[@node-type='feed_list_content'])")
            web_item['内容'] = ''.join([c for c in content if c not in string.whitespace])  # 微博内容

            date_time = div.xpath("string(.//div[@class='content']/p[@class='from']/a[1])")  # 发微博的时间
            date_time = ''.join([t for t in date_time if t not in string.whitespace])  # 发微博的时间
            if len(date_time) == 11:
                date_time = str(datetime.datetime.now())[0:4] + '-' + date_time[0:2] + '-' + date_time[3:5] + ' ' + date_time[-5:]
            else:
                date_time = date_time[0:4] + '-' + date_time[5:7] + '-' + date_time[8:10] + ' ' + date_time[-5:]
            web_item['时间'] = date_time
            pl = div.xpath("string (.//div[@class='card-act']/ul/li[3])")
            web_item['评论数'] = re.findall('\d+', pl)[0] if re.findall('\d+', pl) else 0
            zf = div.xpath("string(.//div[@class='card-act']/ul/li[2])")
            web_item['转发数'] = re.findall('\d+', zf)[0] if re.findall('\d+', zf) else 0
            dz = div.xpath("string(.//div[@class='card-act']/ul/li[4])")
            web_item['点赞数'] = re.findall('\d+', dz)[0] if re.findall('\d+', dz) else 0
            web_item['微博链接'] = 'https:' + div.xpath(".//div[@class='content']/p[@class='from']/a[1]/@href")[0]
            web_item['表情数'] = len(div.xpath(".//div[@class='content']/p//img[@class='face']"))
            web_item['表情'] = '\n'.join(div.xpath(".//div[@class='content']/p//img[@class='face']/@title"))
            web_item['@数'] = content.count('@')
            web_item['@话题数'] = content.count('#') / 2
            p = re.compile(r'[#](.*?)[#]', re.S)
            web_item['@话题'] = '\n'.join(p.findall(content))

            try:
                em = div.xpath(".//div[@class='content']/p[@node-type='feed_list_content_full']/a/i/text()")[0] \
                    if is_full else div.xpath(".//div[@class='content']/p[@node-type='feed_list_content']/a/i/text()")[0]
            except IndexError:
                em = ''

            if em == '2':
                location = div.xpath(".//div[@class='content']/p[@node-type='feed_list_content_full']/a/i/../text()")[0].replace('2', '')\
                    if is_full else div.xpath(".//div[@class='content']/p[@node-type='feed_list_content']/a/i/../text()")[0].replace('2', '')
            else:
                location = ''
            web_item['定位'] = location
            # 招聘、粉丝、私信、年薪、培训
            if not any(['招聘' in content, '粉丝' in content, '私信' in content, '年薪' in content, '培训' in content]):
                web_data_list.append(web_item)
            print(web_item)
        return web_data_list


if __name__ == '__main__':
    change_proxy(1)
    # 日期要多加1天
    date_list = ['2019-01-01', '2019-01-02', '2019-01-03', '2019-01-04', '2019-01-05', '2019-01-06', '2019-01-07', '2019-01-08', '2019-01-09', '2019-01-10', '2019-01-11', '2019-01-12', '2019-01-13', '2019-01-14', '2019-01-15', '2019-01-16', '2019-01-17', '2019-01-18', '2019-01-19', '2019-01-20', '2019-01-21', '2019-01-22', '2019-01-23', '2019-01-24', '2019-01-25', '2019-01-26', '2019-01-27', '2019-01-28', '2019-01-29', '2019-01-30', '2019-01-31', '2019-02-01', '2019-02-02', '2019-02-03', '2019-02-04', '2019-02-05', '2019-02-06', '2019-02-07', '2019-02-08', '2019-02-09', '2019-02-10', '2019-02-11', '2019-02-12', '2019-02-13', '2019-02-14', '2019-02-15', '2019-02-16', '2019-02-17', '2019-02-18', '2019-02-19', '2019-02-20', '2019-02-21', '2019-02-22', '2019-02-23', '2019-02-24', '2019-02-25', '2019-02-26', '2019-02-27', '2019-02-28', '2019-03-01', '2019-03-02', '2019-03-03', '2019-03-04', '2019-03-05', '2019-03-06', '2019-03-07', '2019-03-08', '2019-03-09', '2019-03-10', '2019-03-11', '2019-03-12', '2019-03-13', '2019-03-14', '2019-03-15', '2019-03-16', '2019-03-17', '2019-03-18', '2019-03-19', '2019-03-20', '2019-03-21', '2019-03-22', '2019-03-23', '2019-03-24', '2019-03-25', '2019-03-26', '2019-03-27', '2019-03-28', '2019-03-29', '2019-03-30', '2019-03-31', '2019-04-01', '2019-04-02', '2019-04-03', '2019-04-04', '2019-04-05', '2019-04-06', '2019-04-07', '2019-04-08', '2019-04-09', '2019-04-10', '2019-04-11', '2019-04-12', '2019-04-13', '2019-04-14', '2019-04-15', '2019-04-16', '2019-04-17', '2019-04-18', '2019-04-19', '2019-04-20', '2019-04-21', '2019-04-22', '2019-04-23', '2019-04-24', '2019-04-25', '2019-04-26', '2019-04-27', '2019-04-28', '2019-04-29', '2019-04-30', '2019-05-01', '2019-05-02', '2019-05-03', '2019-05-04', '2019-05-05', '2019-05-06', '2019-05-07', '2019-05-08', '2019-05-09', '2019-05-10', '2019-05-11', '2019-05-12', '2019-05-13', '2019-05-14', '2019-05-15', '2019-05-16', '2019-05-17', '2019-05-18', '2019-05-19', '2019-05-20', '2019-05-21', '2019-05-22', '2019-05-23', '2019-05-24', '2019-05-25', '2019-05-26', '2019-05-27', '2019-05-28', '2019-05-29', '2019-05-30', '2019-05-31', '2019-06-01', '2019-06-02', '2019-06-03', '2019-06-04', '2019-06-05', '2019-06-06', '2019-06-07', '2019-06-08', '2019-06-09', '2019-06-10', '2019-06-11', '2019-06-12', '2019-06-13', '2019-06-14', '2019-06-15', '2019-06-16', '2019-06-17', '2019-06-18', '2019-06-19', '2019-06-20', '2019-06-21', '2019-06-22', '2019-06-23', '2019-06-24', '2019-06-25', '2019-06-26', '2019-06-27', '2019-06-28', '2019-06-29', '2019-06-30', '2019-07-01', '2019-07-02', '2019-07-03', '2019-07-04', '2019-07-05', '2019-07-06', '2019-07-07', '2019-07-08', '2019-07-09', '2019-07-10', '2019-07-11', '2019-07-12', '2019-07-13', '2019-07-14', '2019-07-15', '2019-07-16', '2019-07-17', '2019-07-18', '2019-07-19', '2019-07-20', '2019-07-21', '2019-07-22', '2019-07-23', '2019-07-24', '2019-07-25', '2019-07-26', '2019-07-27', '2019-07-28', '2019-07-29', '2019-07-30', '2019-07-31', '2019-08-01', '2019-08-02', '2019-08-03', '2019-08-04', '2019-08-05', '2019-08-06', '2019-08-07', '2019-08-08', '2019-08-09', '2019-08-10', '2019-08-11', '2019-08-12', '2019-08-13', '2019-08-14', '2019-08-15', '2019-08-16', '2019-08-17', '2019-08-18', '2019-08-19', '2019-08-20', '2019-08-21', '2019-08-22', '2019-08-23', '2019-08-24', '2019-08-25', '2019-08-26', '2019-08-27', '2019-08-28', '2019-08-29', '2019-08-30', '2019-08-31', '2019-09-01', '2019-09-02', '2019-09-03', '2019-09-04', '2019-09-05', '2019-09-06', '2019-09-07', '2019-09-08', '2019-09-09', '2019-09-10', '2019-09-11', '2019-09-12', '2019-09-13', '2019-09-14', '2019-09-15', '2019-09-16', '2019-09-17', '2019-09-18', '2019-09-19', '2019-09-20', '2019-09-21', '2019-09-22', '2019-09-23', '2019-09-24', '2019-09-25', '2019-09-26', '2019-09-27', '2019-09-28', '2019-09-29', '2019-09-30', '2019-10-01', '2019-10-02', '2019-10-03', '2019-10-04', '2019-10-05', '2019-10-06', '2019-10-07', '2019-10-08', '2019-10-09', '2019-10-10', '2019-10-11', '2019-10-12', '2019-10-13', '2019-10-14', '2019-10-15', '2019-10-16', '2019-10-17', '2019-10-18', '2019-10-19', '2019-10-20', '2019-10-21', '2019-10-22', '2019-10-23', '2019-10-24', '2019-10-25', '2019-10-26', '2019-10-27', '2019-10-28', '2019-10-29', '2019-10-30', '2019-10-31', '2019-11-01', '2019-11-02', '2019-11-03', '2019-11-04', '2019-11-05', '2019-11-06', '2019-11-07', '2019-11-08', '2019-11-09', '2019-11-10', '2019-11-11', '2019-11-12', '2019-11-13', '2019-11-14', '2019-11-15', '2019-11-16', '2019-11-17', '2019-11-18', '2019-11-19', '2019-11-20', '2019-11-21', '2019-11-22', '2019-11-23', '2019-11-24', '2019-11-25', '2019-11-26', '2019-11-27', '2019-11-28', '2019-11-29', '2019-11-30', '2019-12-01', '2019-12-02', '2019-12-03', '2019-12-04', '2019-12-05', '2019-12-06', '2019-12-07', '2019-12-08', '2019-12-09', '2019-12-10', '2019-12-11', '2019-12-12', '2019-12-13', '2019-12-14', '2019-12-15', '2019-12-16', '2019-12-17', '2019-12-18', '2019-12-19', '2019-12-20', '2019-12-21', '2019-12-22', '2019-12-23', '2019-12-24', '2019-12-25', '2019-12-26', '2019-12-27', '2019-12-28', '2019-12-29', '2019-12-30', '2019-12-31']

    key_list = ['大盘', 'A股', '股票', '股票市场', '证券', '证券市场']

    for key in key_list:
        for d in date_list:
            # 保存第一页数据，并修改总页数（只能通过第一次请求获取总页数）
            wb = WbSpider(keyword=key, start_time=d + '-0', end_time=d + '-24', page=1)
            data = wb.get_web_data(5)
            save_to_csv(file_name=key, list_dict=data)
            print('############################')
            print(f"{key} {d}第{1}页数据存储成功")
            print('############################')

            # 保存剩下页数数据
            for i in range(2, total_page + 1):
                wb = WbSpider(keyword=key, start_time=d + '-0', end_time=d + '-24', page=i)
                data = wb.get_web_data(5)
                save_to_csv(file_name=key, list_dict=data)
                print('############################')
                print(f"{key} {d}第{i}页数据存储成功")
                print('############################')
